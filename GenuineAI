data = read.csv('reviews.csv')


# install.packages(c("dplyr", "stringr", "textclean", "tidytext", "textstem"))

########################
# 1. DATA CLEANING
########################

####################### STEP 1 ################################

# Dataset structure

# Remove duplicates

# Handle missing values (drop or impute depending on field)

# Standardize text encoding (UTF-8)

library(dplyr)

data_clean <- data %>%
  distinct() %>%                                # remove full duplicates
  filter(!is.na(text) & text != "") %>%         # remove empty reviews
  mutate(across(c(business_name, author_name, rating_category), 
                ~ trimws(.)))                   # strip extra spaces

####################### STEP 2 ################################
# Text cleaning

# Lowercase

# Remove punctuation, numbers, control characters

# Expand contractions

# Remove stopwords

# (Optionally) Lemmatize or stem

library(stringr)
library(textclean)
library(tidytext)
library(textstem)

# Base text cleaning
data_clean <- data_clean %>%
  mutate(
    text_clean = text %>%
      str_to_lower() %>%                       # lowercase
      replace_contraction() %>%                # don't -> do not
      replace_number(remove = TRUE) %>%        # drop numbers
      str_replace_all("[[:punct:]]", " ") %>%  # remove punctuation
      str_replace_all("[[:cntrl:]]", " ") %>%
      str_squish()
  )

# Tokenization & stopword removal
data_tokens <- data_clean %>%
  unnest_tokens(word, text_clean) %>%
  anti_join(stop_words, by = "word") %>%
  filter(str_detect(word, "[a-z]"))   # keep alphabetic only

# Optional: lemmatize words
data_tokens <- data_tokens %>%
  mutate(word = lemmatize_words(word))

####################### STEP 3 ################################

# Feature engineering (for ML downstream)

# Since your policies include “ads / irrelevant / rant detection”, you should also create flags from text:

data_clean <- data_clean %>%
  mutate(
    has_url   = str_detect(text, "http[s]?://|www\\."),
    has_email = str_detect(text, "[[:alnum:]._%+-]+@[[:alnum:].-]+"),
    has_phone = str_detect(text, "\\d{3,}[-.\\s]?\\d*"),  # rough pattern
    text_len  = str_count(text, "\\w+"),                  # word count
    has_profanity = str_detect(text, "(damn|shit|fuck)")   # extend lexicon
  )


####################### STEP 4 ################################

# Handling the photo column

# You can flag whether a review has an attached photo (evidence of visit):

data_clean <- data_clean %>%
  mutate(has_photo = !is.na(photo) & photo != "")


####################### STEP 5 ################################

# Save cleaned dataset

write.csv(data_clean, "reviews_clean.csv", row.names = FALSE)
data_clean <- readRDS("reviews_clean.rds")
save(data_clean, file = "reviews_clean.RData")
load("reviews_clean.RData")



########################
# 2. FEATURE ENGINEERING
########################

# Required packages
library(tidyverse)
library(tidytext)
library(syuzhet)
library(tidyr)

# Add review_id
data_clean$review_id <- 1:nrow(data_clean)

# 1. Metadata features
data_clean <- data_clean %>%
  mutate(
    char_count = nchar(text_clean),
    word_count = str_count(text_clean, "\\S+")
  )

# 2. Sentiment score (Bing method)
data_clean$sentiment <- get_sentiment(data_clean$text_clean, method = "bing")

# 3. TF-IDF features
tokens <- data_clean %>%
  unnest_tokens(word, text_clean) %>%
  anti_join(stop_words, by = "word")

tfidf <- tokens %>%
  count(review_id, word) %>%
  bind_tf_idf(word, review_id, n)

# Wide TF-IDF matrix
tfidf_wide <- tfidf %>%
  select(review_id, word, tf_idf) %>%
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0)

# Combine with metadata
final_data <- data_clean %>%
  left_join(tfidf_wide, by = "review_id")



##################
# 3. MODELING
##################

library(caret)
library(randomForest)

# Ensure target column exists — create synthetic labels if missing
# (remove this part if you already have true labels!)
if (!"policy_violation" %in% colnames(final_data)) {
  set.seed(123)
  final_data$policy_violation <- sample(
    c(0,1),
    size = nrow(final_data),
    replace = TRUE,
    prob = c(0.8, 0.2)
  )
}

# Convert to factor
final_data$policy_violation <- as.factor(final_data$policy_violation)

# Handle NAs
features <- final_data %>%
  select(-review_id, -text_clean, -policy_violation) %>%
  mutate_all(~replace(., is.na(.), 0))

target <- final_data$policy_violation

# Train-test split
set.seed(123)
trainIndex <- createDataPartition(target, p = 0.8, list = FALSE)

train_data <- features[trainIndex, ]
train_target <- target[trainIndex]

test_data <- features[-trainIndex, ]
test_target <- target[-trainIndex]

# Train Random Forest
set.seed(123)
rf_model <- randomForest(
  x = train_data,
  y = as.factor(train_target),
  ntree = 200,
  importance = TRUE
)

print(rf_model)

# Evaluate
predictions <- predict(rf_model, newdata = test_data)

conf_matrix <- confusionMatrix(predictions, as.factor(test_target))
print(conf_matrix)

# Feature importance plot
varImpPlot(rf_model)


library(randomForest)
library(caret)

# Train with class weights (heavier penalty on minority class "1")
rf_model_weighted <- randomForest(
  x = train_data,
  y = as.factor(train_target),
  ntree = 200,
  importance = TRUE,
  classwt = c("0" = 1, "1" = 5)  # Adjust weights (tune 5 higher/lower)
)

# Predict
predictions_weighted <- predict(rf_model_weighted, newdata = test_data)

# Evaluate
conf_matrix_weighted <- confusionMatrix(predictions_weighted, as.factor(test_target))
print(conf_matrix_weighted)
